import re
import requests
import time
import random
import os  # æ–°å¢ï¼šå¤„ç†æ–‡ä»¶å¤¹/è·¯å¾„
from bs4 import BeautifulSoup


def extract_outer_id_and_token(data):
    """
    å…ˆå‰”é™¤note_cardåŠå…¶å†…éƒ¨å†…å®¹ï¼Œå†æå–æœ€å¤–å±‚çš„idå’Œxsec_token
    :param data: åŸå§‹JSONå­—ç¬¦ä¸²æ•°æ®
    :return: æå–åˆ°çš„(id, xsec_token)åˆ—è¡¨
    """
    note_card_pattern = r'"note_card":\s*\{(?:[\s\S]*?)\}'
    cleaned_data = re.sub(note_card_pattern, '', data, flags=re.DOTALL)
    target_pattern = r'\{\s*"id":\s*"([^"]+)"[,}].*?"xsec_token":\s*"([^"]+)"'
    matches = re.findall(target_pattern, cleaned_data, flags=re.DOTALL)
    return matches


# ========== åˆå¹¶åçš„æ ¸å¿ƒå‡½æ•° ==========
def save_content(desc_text, image_url, headers, root_data_path="data"):
    """
    åˆå¹¶æ–‡æœ¬ä¿å­˜+å›¾ç‰‡è®¿é—®/ä¸‹è½½é€»è¾‘ï¼š
    1. åœ¨root_data_pathä¸‹åˆ›å»ºåºåˆ—æ–‡ä»¶å¤¹ï¼ˆdata_1ã€data_2...ï¼‰
    2. ä¿å­˜æè¿°æ–‡æœ¬åˆ°è¯¥æ–‡ä»¶å¤¹çš„txtæ–‡ä»¶
    3. è®¿é—®å¹¶ä¸‹è½½å›¾ç‰‡åˆ°è¯¥æ–‡ä»¶å¤¹
    :param desc_text: æå–åˆ°çš„æè¿°æ–‡æœ¬
    :param image_url: æå–åˆ°çš„å›¾ç‰‡URL
    :param headers: è¯·æ±‚å¤´ï¼ˆå’Œç½‘é¡µè®¿é—®ä¸€è‡´ï¼‰
    :param root_data_path: æ ¹æ•°æ®æ–‡ä»¶å¤¹ï¼ˆé»˜è®¤å·¥ç¨‹ä¸‹çš„dataï¼‰
    """
    # ç¡®ä¿æ ¹dataæ–‡ä»¶å¤¹å­˜åœ¨
    if not os.path.exists(root_data_path):
        os.makedirs(root_data_path)
        print(f"ğŸ“ æ ¹æ–‡ä»¶å¤¹ {root_data_path} ä¸å­˜åœ¨ï¼Œå·²åˆ›å»º")

    # ç”Ÿæˆåºåˆ—æ–‡ä»¶å¤¹å
    existing_folders = []
    for folder in os.listdir(root_data_path):
        folder_path = os.path.join(root_data_path, folder)
        if os.path.isdir(folder_path) and folder.startswith("data_"):
            try:
                folder_num = int(folder.split("_")[1])
                existing_folders.append(folder_num)
            except (IndexError, ValueError):
                continue

    new_folder_num = max(existing_folders) + 1 if existing_folders else 1
    new_folder_name = f"data_{new_folder_num}"
    new_folder_path = os.path.join(root_data_path, new_folder_name)

    # åˆ›å»ºæ–°åºåˆ—æ–‡ä»¶å¤¹
    try:
        os.makedirs(new_folder_path)
        print(f"\nğŸ“ å·²åˆ›å»ºåºåˆ—æ–‡ä»¶å¤¹ï¼š{new_folder_path}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºåºåˆ—æ–‡ä»¶å¤¹å¤±è´¥ï¼š{str(e)}")
        return

    # ä¿å­˜æè¿°æ–‡æœ¬
    txt_file_path = os.path.join(new_folder_path, "description.txt")
    try:
        with open(txt_file_path, 'w', encoding='utf-8') as f:
            f.write(desc_text)
        print(f"ğŸ“ æè¿°æ–‡æœ¬å·²ä¿å­˜åˆ°ï¼š{txt_file_path}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æè¿°æ–‡æœ¬å¤±è´¥ï¼š{str(e)}")

    # ä¸‹è½½å›¾ç‰‡
    if not image_url:
        print("âŒ æ— å›¾ç‰‡URLï¼Œè·³è¿‡å›¾ç‰‡ä¸‹è½½")
        return

    sleep_time = random.uniform(1, 3)
    print(f"\nğŸ–¼ï¸ å‡†å¤‡è®¿é—®å›¾ç‰‡URLï¼š{image_url}")
    print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’åä¸‹è½½...")
    time.sleep(sleep_time)

    # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶å
    img_suffix = image_url.split(".")[-1].split("!")[0]
    if img_suffix not in ["jpg", "png", "webp", "jpeg"]:
        img_suffix = "jpg"
    img_file_name = f"image_{random.randint(1000, 9999)}.{img_suffix}"
    img_file_path = os.path.join(new_folder_path, img_file_name)

    try:
        response = requests.get(image_url, headers=headers, timeout=10, stream=True)
        if response.status_code == 200:
            with open(img_file_path, 'wb') as f:
                for chunk in response.iter_content(1024):
                    f.write(chunk)
            print(f"âœ… å›¾ç‰‡å·²ä¸‹è½½åˆ°ï¼š{img_file_path}")
        else:
            print(f"âŒ å›¾ç‰‡URLè®¿é—®å¤±è´¥ï¼çŠ¶æ€ç ï¼š{response.status_code}")
    except requests.exceptions.Timeout:
        print(f"âŒ å›¾ç‰‡URLè®¿é—®è¶…æ—¶ï¼")
    except requests.exceptions.ConnectionError:
        print(f"âŒ å›¾ç‰‡URLç½‘ç»œè¿æ¥é”™è¯¯ï¼")
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¸‹è½½å‡ºé”™ï¼š{str(e)}")


def insect(urls):
    """
    è®¿é—®URLåˆ—è¡¨ï¼Œæå–descriptionå’Œpreloadå›¾ç‰‡URLï¼Œè°ƒç”¨save_contentä¿å­˜åˆ°åºåˆ—æ–‡ä»¶å¤¹
    :param urls: å°çº¢ä¹¦URLåˆ—è¡¨
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Referer": "https://www.xiaohongshu.com/"
    }

    if not urls:
        print("âŒ æ²¡æœ‰å¯è®¿é—®çš„URLï¼")
        return

    print("\n========== å¼€å§‹è®¿é—®å°çº¢ä¹¦URLï¼ˆæ¨¡æ‹ŸçœŸäººèŠ‚å¥ï¼‰ ==========")
    for idx, url in enumerate(urls, 1):
        # ç½‘é¡µè®¿é—®é—´éš”ï¼ˆ2-5ç§’éšæœºï¼‰
        sleep_time = random.uniform(2, 5)
        print(f"\nã€ç¬¬{idx}ä¸ªURLã€‘: {url}")
        print(f"â³ ç­‰å¾… {sleep_time:.2f} ç§’åè®¿é—®...")
        time.sleep(sleep_time)

        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                print(f"âœ… ç½‘é¡µè®¿é—®æˆåŠŸï¼å“åº”çŠ¶æ€ç ï¼š{response.status_code}")

                # è§£æHTMLæå–ç›®æ ‡å†…å®¹
                soup = BeautifulSoup(response.text, 'lxml')

                # æå–descriptionæ–‡æœ¬
                desc_tag = soup.find('meta', attrs={'name': 'description'})
                desc_text = desc_tag['content'] if (desc_tag and 'content' in desc_tag.attrs) else "æœªæå–åˆ°æè¿°æ–‡æœ¬"
                print(f"\nğŸ“„ æå–åˆ°çš„æè¿°æ–‡æœ¬ï¼š{desc_text}")

                # æå–preloadå›¾ç‰‡URL
                preload_img_tag = soup.find('link', attrs={'rel': 'preload', 'as': 'image'})
                img_url = preload_img_tag['href'] if (preload_img_tag and 'href' in preload_img_tag.attrs) else ""
                if img_url:
                    print(f"ğŸ”— æå–åˆ°çš„é¢„åŠ è½½å›¾ç‰‡URLï¼š{img_url}")
                else:
                    print(f"âŒ æœªæå–åˆ°preloadå›¾ç‰‡URL")

                # è°ƒç”¨åˆå¹¶åçš„å‡½æ•°ï¼Œä¿å­˜æ–‡æœ¬+å›¾ç‰‡åˆ°åºåˆ—æ–‡ä»¶å¤¹
                save_content(desc_text, img_url, headers)

            elif response.status_code == 403:
                print(f"âŒ ç½‘é¡µè®¿é—®è¢«æ‹’ç»ï¼ˆ403ï¼‰ï¼šå»ºè®®å¢å¤§é—´éš”æˆ–æ›´æ¢UA")
            elif response.status_code == 404:
                print(f"âŒ ç½‘é¡µURLæ— æ•ˆï¼ˆ404ï¼‰ï¼š{url}")
            else:
                print(f"âŒ ç½‘é¡µè®¿é—®å¤±è´¥ï¼çŠ¶æ€ç ï¼š{response.status_code}")

        except requests.exceptions.Timeout:
            print(f"âŒ ç½‘é¡µè®¿é—®è¶…æ—¶ï¼")
        except requests.exceptions.ConnectionError:
            print(f"âŒ ç½‘é¡µç½‘ç»œè¿æ¥é”™è¯¯ï¼")
        except Exception as e:
            print(f"âŒ ç½‘é¡µè®¿é—®/è§£æå‡ºé”™ï¼š{str(e)}")


if __name__ == "__main__":
    # 1. è¯»å–æ•°æ®æ–‡ä»¶
    file_path = r"xiaohongshu_data.txt"
    raw_data = ""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_data = f.read()
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ–‡ä»¶ {file_path}ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        # ç¤ºä¾‹æ•°æ®å…œåº•
        raw_data = """
        {
            "id": "67d25423000000000901413b",
            "model_type": "note",
            "note_card": {...},
            "xsec_token": "ABoVoOTZNOT5_vDnmGeWb7XQKF7ZJzMeFTIYEL321cWrM="
        },
        {
            "id": "66e98abf0000000027004b5a",
            "model_type": "note",
            "note_card": {...},
            "xsec_token": "ABWF6ORf4h2lT2ksZbRC_22EPzQONEczamyJdmVn_T69o="
        }
        """

    # 2. æå–idå’Œxsec_tokenï¼Œæ‹¼æ¥URL
    result = extract_outer_id_and_token(raw_data)
    xhs_urls = []
    if result:
        print("âœ… æå–åˆ°æœ€å¤–å±‚çš„idå’Œxsec_tokenï¼š")
        for idx, (item_id, xsec_token) in enumerate(result, 1):
            print(f"\nç¬¬{idx}æ¡ï¼š")
            print(f"id: {item_id}")
            print(f"xsec_token: {xsec_token}")
            xhs_url = f"https://www.xiaohongshu.com/explore/{item_id}?xsec_token={xsec_token}"
            print(f"å°çº¢ä¹¦é“¾æ¥: {xhs_url}")
            xhs_urls.append(xhs_url)
    else:
        print("âŒ æœªæå–åˆ°ä»»ä½•æ•°æ®ï¼")

    # 3. è®¿é—®URLå¹¶ä¿å­˜å†…å®¹åˆ°åºåˆ—æ–‡ä»¶å¤¹
    insect(xhs_urls)